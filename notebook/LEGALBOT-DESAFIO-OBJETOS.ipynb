{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data collection\n",
    "\n",
    "Se realiza la lectura de los objetos directamente desde el github de felipeares."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "url = 'https://raw.githubusercontent.com/felipeares/legalbot-desafio-objetos/master/objetos.txt'\n",
    "page = requests.get(url)\n",
    "#Se identificó visualmente que el separador de los objetos corresponde a los caracteres \\n\\n\n",
    "df_corpus = pd.DataFrame(page.text.split('\\n\\n'), columns = ['objeto'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>objeto</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CONSTRUCCIÓN, ALQUILER DE OTROS TIPOS DE MAQUI...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>el servicio de turismo por todo el territorio ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SALON DE BELLEZA.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CURSOS ONLINE DE MUSICA, COCINA, CLASES DE MUS...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>el ejercicio de la actividad comercial,importa...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              objeto\n",
       "0  CONSTRUCCIÓN, ALQUILER DE OTROS TIPOS DE MAQUI...\n",
       "1  el servicio de turismo por todo el territorio ...\n",
       "2                                  SALON DE BELLEZA.\n",
       "3  CURSOS ONLINE DE MUSICA, COCINA, CLASES DE MUS...\n",
       "4  el ejercicio de la actividad comercial,importa..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_corpus.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\jcartesf\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\jcartesf\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\jcartesf\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string  #Se obtendrá los signos de puntuación\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer, SnowballStemmer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Método limpieza inicial y tokenización\n",
    "def get_tokens_clean(objeto):\n",
    "    #Se deja el texto en minúscula\n",
    "    objeto = objeto.lower()\n",
    "    \n",
    "    #Elimina tildes\n",
    "    a,b = 'áéíóúü','aeiouu',\n",
    "    trans = str.maketrans(a,b)\n",
    "    objeto = objeto.translate(trans)\n",
    "    \n",
    "    #Elimina caracteres que no sean palabras\n",
    "    filtered_punctuation =  objeto.translate({ord(c): None for c in string.punctuation})\n",
    "    \n",
    "    #tokeniza\n",
    "    tokens = nltk.word_tokenize(filtered_punctuation)\n",
    "    \n",
    "    #Se eliminan stopwords (articulos, preposiciones, etc.)\n",
    "    no_stopwords = [word for word in tokens if word not in stopwords.words('spanish')]\n",
    "       \n",
    "    return no_stopwords\n",
    "\n",
    "#Función de derivacion \n",
    "def stem_words(tokens):\n",
    "    stemmer_spanish = SnowballStemmer('spanish')\n",
    "    stems = []\n",
    "    for word in tokens:\n",
    "        stem = stemmer_spanish.stem(word)\n",
    "        stems.append(stem)\n",
    "    return stems\n",
    "\n",
    "#Elimina palabas menor a tres caracteres\n",
    "def delete_characteres(tokens):\n",
    "    return  [word for word in tokens if len(word)>3]\n",
    "\n",
    "#Lematización de verbos. Por ahora sólo palabras en inglés.\n",
    "def lemmatize_stemming(text):\n",
    "    return WordNetLemmatizer().lemmatize(text, pos='v')\n",
    "\n",
    "#Se eliminan palabras derivadas con alta frecuencia, ya que sesga los grupos.\n",
    "def replace_word_frequence(tokens):\n",
    "    replace_word = []\n",
    "    for word in tokens:\n",
    "        word = word.replace('activ','').replace('servici','')\n",
    "        if word == '': continue\n",
    "        replace_word.append(word)\n",
    "    return replace_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\jcartesf\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\gensim\\utils.py:1212: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "from gensim.corpora import Dictionary\n",
    "\n",
    "df_corpus['objeto_tokens'] = df_corpus['objeto'].map(get_tokens_clean)\n",
    "df_corpus['objeto_tokens'] = df_corpus['objeto_tokens'].map(delete_characteres) \n",
    "df_corpus['objeto_stemm']  = df_corpus['objeto_tokens'].map(stem_words)\n",
    "df_corpus['objeto_stemm'] = df_corpus['objeto_stemm'].map(replace_word_frequence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Se crea diccionario\n",
    "dct = Dictionary(df_corpus['objeto_stemm'].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''import matplotlib.pyplot as plt\n",
    "flat_list = [word for idx,array_word in df_corpus.iterrows() for word in array_word['objeto_stemm']]\n",
    "word_group = pd.DataFrame(flat_list,index=flat_list,columns=['word'])['word']\n",
    "word_group_count = word_group.value_counts()\n",
    "## VENTAS POR MES ####\n",
    "plt.figure(figsize=(15, 15))\n",
    "plt.ion()\n",
    "\n",
    "plt.plot(word_group_count[word_group_count>300], 'b')  # Dibujamos una línea recta azul\n",
    "plt.xlabel('palabras')  # Ponemos etiqueta al eje x\n",
    "plt.ylabel(u'Cantidad')  # Ponemos etiqueta al eje y\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
